{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raLwbLFaRvOR"
   },
   "source": [
    "### **Columbia University**\n",
    "### **ECBM E4040 Neural Networks and Deep Learning. Fall 2021.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJCMCx6ISZZw"
   },
   "source": [
    "## **Task 2: RNN application -- Tweet Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8wjQhNtSffW"
   },
   "source": [
    "In this task, you are going to classify the sentiment in tweets into positive and negative using an LSTM model. The code to load the data and see its characteristics has been provided to you. \n",
    "\n",
    "In the first task, you will encode the data using using one hot encoding and train an LSTM network to classify the sentiment. In the second task, you will replace the one hot encoding with an embedding layer and train another LSTM model. You will then extract the trained embeddings and visualize the word embeddings in 2 dimensions by using TSNE for dimenssionality redution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UZ_G4XdfP7GK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjTYqMoN8fh"
   },
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F55EwI6RQl1A",
    "outputId": "1e087591-3dad-4471-97df-d9af9214dddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of original train set: 60000\n",
      "size of original test set: 20000\n",
      "****************************************************************************************************\n",
      "size of train set: 60000, #positive: 30055, #negative: 29945\n",
      "size of test set: 1000, #positive: 510, #negative: 490\n",
      "['it', 'will', 'help', 'relieve', 'your', 'stress', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken']\n",
      "sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "with open(\"./tweets_data/vocabulary.pkl\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_data = json.load(open('tweets_data/trainTweets_preprocessed.json', 'r'))\n",
    "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
    "train_tweets = np.array([t[0] for t in train_data])\n",
    "train_labels = np.array([int(t[1]) for t in train_data])\n",
    "\n",
    "test_data = json.load(open('tweets_data/testTweets_preprocessed.json', 'r'))\n",
    "test_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data))\n",
    "test_tweets = np.array([t[0] for t in test_data])\n",
    "test_labels = np.array([int(t[1]) for t in test_data])\n",
    "\n",
    "print(\"size of original train set: {}\".format(len(train_tweets)))\n",
    "print(\"size of original test set: {}\".format(len(test_tweets)))\n",
    "\n",
    "# only select first 1000 test sample for test\n",
    "test_tweets = test_tweets[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"size of train set: {}, #positive: {}, #negative: {}\".format(len(train_tweets), np.sum(train_labels), len(train_tweets)-np.sum(train_labels)))\n",
    "print(\"size of test set: {}, #positive: {}, #negative: {}\".format(len(test_tweets), np.sum(test_labels), len(test_tweets)-np.sum(test_labels)))\n",
    "\n",
    "# show text of the idx-th train tweet\n",
    "# The 'padtoken' is used to ensure each tweet has the same length\n",
    "idx = 100\n",
    "train_text = [vocabulary[x] for x in train_tweets[idx]]\n",
    "print(train_text)\n",
    "sentiment_label = [\"negative\", \"positive\"]\n",
    "print(\"sentiment: {}\".format(sentiment_label[train_labels[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vocabulary == \"help\")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10,   53,  221, 2727,   41, 1418,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmjy9sPDOCnY"
   },
   "source": [
    "## **Part 1 LSTM Encoder**\n",
    "\n",
    "**TODO**: Create a single-layer LSTM network to classify tweets. Use one hot encoding to represent each word in the tweet. Set LSTM units to 100. Use Adam optimizer and set batch size to 64.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "\n",
    "With these settings, what accuracy could you get? You can try to change some stuff in the network to see if you could get a better accuracy (this is optional). \n",
    "\n",
    "(tf.one_hot and Keras functional API may be useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"category_encoding_16\" (type CategoryEncoding).\n\nWhen output_mode is not `'int'`, maximum supported output rank is 2. Received output_mode one_hot and input shape (2, 20), which would result in output rank 3.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(2, 20), dtype=int32)\n  • count_weights=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-5204199a9f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategoryEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"one_hot\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\nndl_hw\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nndl_hw\\lib\\site-packages\\keras\\layers\\preprocessing\\preprocessing_utils.py\u001b[0m in \u001b[0;36mencode_categorical_inputs\u001b[1;34m(inputs, output_mode, depth, sparse, count_weights, idf_weights)\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     raise ValueError(\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[1;34mf\"When output_mode is not `'int'`, maximum supported output rank is 2. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;34mf\"Received output_mode {output_mode} and input shape {original_shape}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         f\"which would result in output rank {inputs.shape.rank}.\")\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"category_encoding_16\" (type CategoryEncoding).\n\nWhen output_mode is not `'int'`, maximum supported output rank is 2. Received output_mode one_hot and input shape (2, 20), which would result in output rank 3.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(2, 20), dtype=int32)\n  • count_weights=None"
     ]
    }
   ],
   "source": [
    "num_tokens = len(vocabulary)\n",
    "print(train_tweets[:2].shape)\n",
    "l = tf.keras.layers.CategoryEncoding(num_tokens=num_tokens, output_mode=\"one_hot\", sparse=False)\n",
    "l(train_tweets[:2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "-Mx6WgMBVI3T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"category_encoding\" (type CategoryEncoding).\n\nWhen output_mode is not `'int'`, maximum supported output rank is 2. Received output_mode one_hot and input shape (None, 20), which would result in output rank 3.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 20), dtype=float32)\n  • count_weights=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-7735d5b966d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# cat = tf.keras.layers.CategoryEncoding(num_tokens=num_tokens, output_mode=\"one_hot\", sparse=False)(input_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nndl_hw\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-7735d5b966d5>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# cat = tf.keras.layers.CategoryEncoding(num_tokens=num_tokens, output_mode=\"one_hot\", sparse=False)(input_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-7735d5b966d5>\u001b[0m in \u001b[0;36mone\u001b[1;34m(in_, num_tokens)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategoryEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"one_hot\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"category_encoding\" (type CategoryEncoding).\n\nWhen output_mode is not `'int'`, maximum supported output rank is 2. Received output_mode one_hot and input shape (None, 20), which would result in output rank 3.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 20), dtype=float32)\n  • count_weights=None"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# TODO: Create a single-layer LSTM network.       #\n",
    "#                                                 #\n",
    "###################################################\n",
    "num_tokens = len(vocabulary)\n",
    "\n",
    "def one(in_, num_tokens):\n",
    "    return tf.keras.layers.CategoryEncoding(num_tokens=num_tokens, output_mode=\"one_hot\", sparse=False)(in_)\n",
    "\n",
    "input = tf.keras.layers.Input((20,))\n",
    "print(input.shape)\n",
    "# cat = tf.keras.layers.CategoryEncoding(num_tokens=num_tokens, output_mode=\"one_hot\", sparse=False)(input_)\n",
    "x = tf.keras.layers.Lambda(lambda i: one(i, num_tokens))(input)\n",
    "X = tf.keras.layers.LSTM(100)(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "den2 = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(input, den2)\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_tweets, \n",
    "                        train_labels, batch_size=64, epochs=50, validation_data=(test_tweets, test_labels))\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBiLRGc7RL-g"
   },
   "source": [
    "## **Part 2: Embedding Lookup layer**\n",
    "\n",
    "**Define an embedding layer**\n",
    "\n",
    "It's not hard to imagine in the previous practices, the input we fed in are very sparse because each word was represented as a one-hot vector. This makes it difficult for the network to understand what story the input data is telling.\n",
    "\n",
    "Word embedding: instead of using a one-hot vector to represent each word, we can add an word embedding matrix in which each word is represented as a low-dimensional vector. Note that this representation is not sparse any more, because we're working in a continuous vector space now. Words that share similar/related semantic meaning should be 'close to each other' in this vector space (we could define a distance measure to estimate the closeness).\n",
    "\n",
    "**TODO**: Define a similar model as above with one change. Use an Embedding layer instead of one hot embedding. Also, write a custom training loop to train the model instead of using model.fit(). Writing a custom loop gives you complete control over how the model is trained. Refer to the link below.\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
    "\n",
    "Report loss and accuracy for training and validation after each epoch. Also, display the loss value after every 400 steps. \n",
    "\n",
    "Do you see any difference in accuracy? What about training time? What inference can you draw?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiDynRCc_mqN"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_tweets, train_labels))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test_tweets, test_labels))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "x2mkQlVMVUny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20)\n",
      "(None, 20, 128)\n",
      "(None, 1)\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.7483\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.6929\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.6937\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.4975\n",
      "Validation acc: 0.4879\n",
      "Time taken: 32.15s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.6931\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.6788\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.6371\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.6269\n",
      "Validation acc: 0.6002\n",
      "Time taken: 28.33s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.6879\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.5999\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.5941\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.7271\n",
      "Validation acc: 0.7328\n",
      "Time taken: 53.79s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.4981\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.5689\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.5492\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.7733\n",
      "Validation acc: 0.7535\n",
      "Time taken: 18.96s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.4690\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.5338\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.5000\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.7924\n",
      "Validation acc: 0.7623\n",
      "Time taken: 19.88s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.4506\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.4573\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.4904\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.8065\n",
      "Validation acc: 0.7672\n",
      "Time taken: 20.65s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.4077\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.4709\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.4509\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.8171\n",
      "Validation acc: 0.7633\n",
      "Time taken: 21.15s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.3787\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.4481\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.5203\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.8242\n",
      "Validation acc: 0.7604\n",
      "Time taken: 22.05s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.3444\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.4609\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.4392\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.8335\n",
      "Validation acc: 0.7568\n",
      "Time taken: 21.64s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.3581\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 400: 0.4644\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 800: 0.4096\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.8383\n",
      "Validation acc: 0.7480\n",
      "Time taken: 20.60s\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# TODO: Create a single-layer LSTM network        #\n",
    "#       using Embedding layer                     #\n",
    "###################################################\n",
    "vocab_len = len(vocabulary)\n",
    "embed_dim = 128\n",
    "lstm_out = 100\n",
    "\n",
    "inputs = tf.keras.Input((20,))\n",
    "print(input.shape)\n",
    "embed = tf.keras.layers.Embedding(vocab_len, embed_dim, input_length=train_tweets.shape[1])(inputs)\n",
    "print(embed.shape)\n",
    "x = tf.keras.layers.SpatialDropout1D(0.4)(embed)\n",
    "# print(x.shape)\n",
    "x = tf.keras.layers.LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "# print(x.shape)\n",
    "x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "# print(x.shape)\n",
    "x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "# print(x.shape)\n",
    "den = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "print(den.shape)\n",
    "\n",
    "model = tf.keras.Model(inputs, den)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # Log every 400 batches.\n",
    "        if step % 400 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9WYOKJtUwe5"
   },
   "source": [
    "## **TODO:**  **Visualize word vectors via tSNE**\n",
    "\n",
    "First, you need to retrieve embedding matrix from the network. Then use tSNE to reduce each low-dimensional word vector into a 2D vector.\n",
    "\n",
    "And then, you should visualize some interesting word pairs in 2D panel. You may find scatter function in matplotlib.pyplot useful.\n",
    "\n",
    "Hint: You can use TSNE tool provided in scikit-learn. And if you encounter dead kernel problem caused by \"Intel MKL FATAL ERROR: Cannot load libmkl_avx.so or libmkl_def.so\", please reinstall scikit-learn without MKL, ie., conda install nomkl numpy scipy scikit-learn numexpr.\n",
    "\n",
    "Here we provide some word pairs for you, like female-male or country-capital. And you can observe that these word-pair will look parallel with each other in a 2D tSNE panel. And you can find some other words and explore their relationship.\n",
    "\n",
    "The result for female-male pairs should look like, and you will observe that king-men and queen-women are parallel to each other in a 2D panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "aVZwhreIVlk0"
   },
   "outputs": [],
   "source": [
    "word_embeddings = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7597, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random').fit_transform(word_embeddings)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-bright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_embedded[np.where(vocabulary == \"spain\")[0][0]], X_embedded[np.where(vocabulary == \"madrid\")[0][0]], label='spain - madrid')\n",
    "plt.plot(X_embedded[np.where(vocabulary == \"japan\")[0][0]], X_embedded[np.where(vocabulary == \"tokyo\")[0][0]], label='japan - tokyo')\n",
    "plt.title('2d embedding projection')\n",
    "plt.ylabel('x2')\n",
    "plt.xlabel('x1')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Assignment_3_task_2_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
